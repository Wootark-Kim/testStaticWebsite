<!DOCTYPE html>

<html lang="en">

	<head>
		<meta charset="UTF-8">
		<title> CST 300 </title>
		<link href="css/styles.css" rel="stylesheet">
	</head>

	<body>
		<header> 
			<span style="font-size: 30px;"> Wootark Kim </span>
			<br><br>
			<span style="font-size: 20px;"> Courses Portfolio </span>
			<br><br>

			<nav>
				<a href="home.html"> Home </a>
				<a href="index.html"> Courses </a>
			</nav>
		</header>

		<main>
			<br>
			<b>CST 300 - Major Proseminar</b> <br><br>
			<div class="course_content">
				"Helps students identify and articulate personal, professional, and social goals. Provides an 
				integrated overview of the computer science and communication design majors and their requirements. 
				Students develop a plan for their learning goals. Students learn writing, presentation, research 
				and critical-thinking skills within the diversified fields of information technology and 
				communication design." <br><br>

				Description from: <br>

				CST 300 - Major ProSeminar - Modern Campus Catalog™
			</div> <!-- end of ".course_content" -->

			<br><br>
				<iframe src="https://www.youtube.com/embed/uAP4zkGJocQ?si=tToMMTXQNJ82VWVn"> </iframe>
			<br><br>

			<b>Ethics Argument Essay</b><br><br>
			
			<div class="course_content">
				Wootark Kim <br>
				CST 300 <br>
				22 February 2025 <br>

				<div style="text-align: center"> Ethics of AI Filtering Candidates </div>				

				<b>Introduction</b>
				<p>
					Days of manually reading each application have quickly become a practice of the past. Companies are now turning to artificial intelligence (AI) driven screening tools to expedite and streamline the hiring process. Examples include resume parsers and online behavioral or skills assessments which promise to save time and reduce human error. Such widespread adoption of these tools also sparked debate regarding their fairness and effectiveness. While AI can help manage the influx of applications, it has raised alarming concerns about AI displaying bias and evaluating candidates incompletely. AI systems, after all, are often trained using data from real-world hiring practices, which may carry inherent biases. Moreover, AI’s focus on specific keywords or set parameters may prevent it from assessing a candidate’s qualifications in a holistic way. To delve into this matter further, the question at hand is: “Should AI driven screening tools be used to filter candidates’ job applications?” In answering this question, it is essential to recognize the stakeholders involved: recruiters and job candidates. Rather than being mutually exclusive, both recruiters and job candidates hold interests on both sides of the debate.
				</p>

				<b>Stakeholder Analysis</b>
				<p>
				The first group of stakeholders are those who support the usage of AI screening tools. As mentioned earlier, both the recruiters and the candidates themselves have reasons for supporting this side. The support stems from the needs that the AI tools fulfill, primarily speed and efficiency, values that both the stakeholders appreciate. Recruiters believe time and resources should not be wasted on mundane tasks such as creating a query of relevant keywords and manually reading through candidates’ resumes. Meanwhile, job candidates appreciate a faster process, believing they should not wait for weeks to months for a response all the while adhering to the schedule of the recruiters for interviews.
				</p>

				<p>
					These stakeholders support the usage of AI screening tools as they believe AI will eliminate such problems. Recruiters state they can speed up their workflow by allowing AI to suggest relevant keywords, filter through hundreds of candidates, and remove recruiters’ bias (Li et al., 2021). Applicants, on the other hand, state AI assisted screening will allow faster response time and reduce barriers that stem from human interaction such as stunted answers caused by anxiety (Horodyski, 2023)
				</p>

				<p>
					The claims made by recruiters and candidates can be categorized into two types of arguments. Recruiters' claims align with a claim of fact, as the collective experiences of multiple recruiters verifies that AI screening tools speed up workflows by assisting with keyword searches, filtering resumes, and reducing biases in the hiring process (Li et al., 2021). Additionally, candidates' claims reflect a claim of cause, since they suggest the comfort of faster response times and reduce barriers caused by real-time human interactions, such as interview anxiety, can lead to more thoughtful and prepared answers (Horodyski, 2023).
				</p>

				<p>
					While the first group of stakeholders supports AI for its efficiency and speed, the second group opposes it for reasons rooted in ethics and fairness. This group includes, once again, both recruiters and job candidates, though it tends to feature more experienced veteran recruiters and candidates who are more socially inclined. For these stakeholders, values such as transparency and the availability of human judgment are fundamental. They believe that hiring decisions should be based on human expertise, which considers nuance and context, rather than solely relying on automated systems. This perspective reflects their desire for fairness, inclusivity, and a hiring process that takes into account the complex qualities and potential of candidates that AI may overlook.
				</p>

				<p>
					These stakeholders are against the usage of AI in screening tools because they believe the technology leads to oversimplification and bias in hiring processes. They argue that AI tools prioritize candidates’ credentials, causing recruiters to focus on meeting qualifications or “check-boxing,” which often overlooks candidates who might possess qualities valuable in the long term but don’t fit predefined criteria (Heaslip, 2024). Moreover, AI’s inability to consider unique circumstances or experiences removes nuance and judgment from the hiring process, particularly in complex situations where human empathy, reasoning, and judgment are crucial (Horodyski, 2023). As such, AI screening tools have been criticized for their low accuracy and reliability, as they often rely on training data from the internet, which may be flawed or biased (Horodyski, 2023). This is concerning because AI may fail to adapt to the subtleties of human experience, making it less effective in fair decision-making. Lastly, opponents highlight that AI systems are often not transparent, making it hard to understand how decisions are made or to challenge potentially discriminatory outcomes. These technical issues compound the concerns about perpetuating discrimination or inaccuracies in hiring practices (Li et al., 2021).
				</p>

				<b>Stakeholder Argument</b>

				<p>
					The first stakeholders (those in support) follow the principles of Ethical Egoism. Stated by Henry Sidgwick in his book The Methods of Ethics (1907), it dictates that decisions made by self-interest are moral since all our actions are inherently defined by self-interest anyway. Stakeholders, which includes both recruiters and job candidates, who use or support AI do it for the perceived good and benefits it brings to them or for their organization. So what benefits are included?
				</p>
				
				<p>
					Recruiters use it mainly for time efficiency and retention. Using AI talent sourcing software eliminates the need for recruiters to create complex queries by themselves to source candidates. The AI will offer keyword suggestions by generating additional related terminologies related to the field that recruiters can use to search for employees (Li et al., 2021). In addition, AI can design the job advertisement with relevant requirements that the recruiter not experienced in a particular industry may otherwise not know (Chen, 2023a). Even contacting candidates has been simplified as AI can recommend candidates and identify their contact information (Li et al., 2021). Use of AI talent sourcing software also does not require substantive training for recruiters to use. This in turn helps with the retention rate of recruiter themselves as well since “(AI) help(s) with the skill shortage on our end, in terms of having folks that have an interest and are willing to work hard enough to then be able to qualify folks” (Li et al., 2021, p. 170).
				</p>

				<p>
					Some candidates support the implementation of AI screening tools, as AI can help reduce stress for job applicants by shortening response times and removing the anxiety associated with human interaction during interviews (Horodyski, 2023). This is because applicants no longer have to wait for multiple weeks or months for a human to review their applications. In addition, uploading video interviews provides applicants with ample time and control to prepare for their response. AI screening of such videos can remove the need for in-person interviews, thus expediting the hiring process for the candidate.
				</p>

				<p>
					The second stakeholders, which include veteran recruiters and more socially inclined candidates, follow the principles of utilitarianism (or utility). Mentioned by Jeremy Bentham’s Principles of Morals and Legislation (1789), this ethical framework focuses on the effects of every party involved, asserting that the moral choice is the one that produces the greatest benefit and the least harm to everyone affected. Stakeholders against AI are driven by the unequal negative effect it produces. Data and algorithmic discrimination, inadequate transparency, and lack of human control are too substantial to accept as tolerable for the ease AI brings.
				</p>

				<p>
					In applying utilitarianism, veteran recruiters and socially inclined candidates are concerned that AI recruitment tools, rather than producing an overall benefit, may lead to systemic harm. These stakeholders argue that AI does not inherently bring the greatest benefit to all parties involved, especially in cases where it perpetuates biases. For instance, AI sourcing software may develop its own biases, often unintentionally mirroring the habits of previous recruiters. An example cited by Li et al. (2021) discusses how one recruiter’s “liked” feature, initially designed to prioritize candidates, led to an AI tool that skewed towards recommending candidates from a specific company (Google). This instance highlights a key flaw in AI recruitment: the tool’s tendency to replicate human bias, often without adequate checks. The issue becomes more concerning when, despite recruiters’ tendency to prefer and see human experts’ input as more trustworthy, they still tend to be more swayed and influenced by AI’s algorithmic suggestions (Lacroux, 2022). This means even human input meant to correct bias may become overshadowed by the AI's influence on recruiters
				</p>

				<p>
					Additionally, even if the recruiter is aware of the aforementioned problem, the lack of control and transparency over AI’s algorithm process further exacerbates the situation. The moral concern for these stakeholders is that the recruiters, though responsible for selecting talent, may not have the power to influence the outcome when biased AI algorithms are used. As one recruiter in the study noted, they “did not have precise control over the sourcing tool to mitigate the skewed result” (Li et al., 2021, p. 170). This lack of control poses a risk not only for recruiters but also for the candidates who may be unfairly screened out, even though they could be well-qualified.
				</p>

				<p>
					The risks of algorithmic discrimination are also significant. AI recruitment models are trained using historical data, which, when it lacks representation from underrepresented groups, can result in biased outcomes. Chen (2023b) argues that “inadequate data will screen out groups that have been historically underrepresented in the recruitment process” (p. 3). Such biases in AI models can perpetuate existing inequalities, thereby increasing the harm rather than reducing it for certain groups. If AI recruitment tools become the norm, this could unintentionally institutionalize these biases, creating long-term inequities in the workforce.
				</p>

				<p>
					For veteran recruiters, the use of AI in recruitment threatens their traditional methods, which, while imperfect, often provide more flexibility in addressing individual candidate potential. The sense of human judgment in recruitment cannot easily be replaced by an algorithm that may overlook nuance and context. Similarly, candidates, especially those from underrepresented backgrounds, are at risk of being sidelined in favor of candidates who fit a more limited or biased profile.
				</p>

				<p>
					In the context of utilitarianism, the argument against AI recruitment tools is clear: while these technologies may offer operational efficiency, they do so at the cost of transparency and fairness. The desired outcome for these stakeholders is to regulate or modify the widespread use of AI in recruitment, ensuring that any screening tools implemented serve to enhance, rather than harm, the equal treatment of all individuals involved. The moral imperative, from their perspective, is to prioritize human oversight, inclusivity, and the mitigation of bias, ensuring that the greatest benefit is achieved for the greatest number of people.
				</p>

				<b>Paper's Position</b>

				<p>
					When considering such reasonings, ideally, it is evident that such software should not be primarily used to source and assess candidates for job applications. While AI can expedite the hiring process, it sacrifices reliability, accuracy, and fairness.
				</p>

				<p>
					This sentiment is in line with the stakeholders against or limiting AI screening tools. By reducing human control in assessing candidates, AI systems are prone to replicating biases and oversimplifying the hiring process, ultimately causing greater harm than good to both recruiters and candidates. Recruiters lose the ability to control bias and make nuanced decisions, while candidates, particularly those from underrepresented groups, may be unfairly excluded due to the limitations of AI technology.
				</p>

				<p>
					To address these issues, several changes are recommended for the current use of AI in recruitment. First, there must be greater transparency regarding the algorithms in use and how they are trained, enabling both recruiters and candidates to understand how AI is making its decision and to reduce the risk of unconscious bias in AI-driven assessments (Chen, 2023a). Second, recruiters should have more direct access and control over the AI tools, allowing for intervention and adjustment of algorithms to prevent biases from influencing outcomes. Finally, AI software must evolve beyond simply scanning for keywords or required credentials. To make better hiring decisions, AI should be capable of holistically assessing a candidate's qualifications, experiences, and potential, ensuring that the right fit is identified based on a comprehensive view of the candidate.
				</p>

				<p>
					Ultimately, while AI holds the potential to enhance recruitment processes, it must be used thoughtfully, with adequate oversight and continuous improvement, to prevent biases and ensure it complements, rather than replaces, human judgment in hiring decisions.
				</p>
			</div> <!-- end of ".course_content" -->
		</main>

		<footer>
			<img id="logo" src="img/logo.png" alt="image of csumb logo"><br>
			CST336 Internet Programming. 2025; Kim <br><br><br>
		</footer>
	</body>